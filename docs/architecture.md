

# MCP over Linkerd â€“Â Architecture Guide
> âš™ï¸ **Audience:** another AI or engineer that will implement the system.  
> ðŸŽ¯ **Scope:** describes every runtime component, their contracts, failureâ€‘handling and deployment footprint.  

---

## 1. Design Principles

| Principle | Consequence |
|-----------|-------------|
| **Authoritative data stays where it already lives** | â€¢ Linkerd topologyâ€¯â†’â€¯Kubernetes CRDs + Destination API  <br>â€¢ Traffic volumesâ€¯â†’â€¯Prometheus  |
| **Zero heavy datastores** | Only an inâ€‘memory graph plus an *optional* Valkey/Redis cache are introduced. |
| **Clear producer/consumer split** | One **Collector** scrapes & publishes; many **MCP Servers** serve the API. |
| **Selfâ€‘healing & fast coldâ€‘start** | Collector snapshots the graph to Redis; any MCP Server can hydrate in <â€¯1â€¯s. |
| **Cheap highâ€‘availability** | Redis keyâ€‘based leader election; k8s informers drive reconvergence. |

---

## 2. Component Topology

```mermaid
graph TD
    subgraph Data Plane
        k8s[Kubernetes API<br/>(CRDs, Pods)]
        prom[Prometheus<br/>(linkerd_* series)]
    end

    subgraph Control Plane
        redis[(Redis / Valkey)]
        Collector
        MCP1[MCPâ€‘Server #1]
        MCP2[MCPâ€‘Server #2]
    end

    k8s -- Informers --> Collector
    prom -- PromQL --> Collector

    Collector -- SETNX/EXPIRE<br/>mesh:snapshot<br/>PUBLISH mesh:delta --> redis

    redis -- GET mesh:snapshot<br/>SUB mesh:delta --> MCP1
    redis -- GET mesh:snapshot<br/>SUB mesh:delta --> MCP2

    MCP1 -- gRPC API --> Clients
    MCP2 -- gRPC API --> Clients

    MCP1 -- Apply/Patch --> k8s
    MCP2 -- Apply/Patch --> k8s
```

---

## 3. Runtime Roles & Responsibilities

### 3.1 Collector (single leader)

| Capability | Details |
|------------|---------|
| **Leader election** | `SETNX mcp:leader $podUID EX 30` every 10â€¯s; followers retry on failure. |
| **Topology ingest** | *K8s Informers* on `Service`, `Pod`, `HTTPRoute`, `GRPCRoute`, `AuthorizationPolicy`, â€¦ |
| **Metrics ingest** | Every **15â€¯s** run:<br>`sum by(src,dst,meshed,tls)(rate(linkerd_request_total[30s]))` |
| **Graph assembly** | Merge informer events + PromQL result into an inâ€‘memory `graph` object. |
| **Event fanâ€‘out** | `PUBLISH mesh:delta <json-patch>` for every change. |
| **Snapshotting** | Every **5â€¯min** gzip+json the full graph â†’ `SET mesh:snapshot â€¦ EX 10m`. |

### 3.2 MCP Server (stateless API layer)

| Capability | Details |
|------------|---------|
| **Warmâ€‘start** | On boot: `GET mesh:snapshot`; if hit â†’ inflate â†’ seed local graph. |
| **Live updates** | `SUBSCRIBE mesh:delta`; apply JSON patches in order. |
| **API surface** | `GetMeshGraph`, `GetCallGraph`, `WatchMeshGraph` (serverâ€‘streaming), `ApplyAuthorizationPolicy`, `ApplyHTTPRoute`. |
| **Mutations** | For `Apply*` calls: `kubectl.Apply()` serverâ€‘dryâ€‘run; if valid â†’ patch live CRD. |
| **RBAC** | mTLS cert â†’ SPIFFE ID â†’ JWT claims; gRPC interceptor checks methodâ€‘level roles. |
| **Observability** | `/metrics` (Promâ€‘format), `/healthz`, `/ready`. |

### 3.3 RedisÂ /Â Valkey (shared cache + lock)

| Key / Channel | Purpose | TTL |
|---------------|---------|-----|
| `mcp:leader`  | leader lock (`$podUID`) | 30â€¯s |
| `mesh:snapshot` | gzipâ€‘JSON full graph | 10â€¯min |
| `mesh:delta` *(pub/sub)* | JSONâ€‘patch deltas | â€” |

No persistence (AOF/RDB) â€“ memoryâ€‘only.

---

## 4. Data Model Sketch

```json
// mesh:snapshot (prettyâ€‘printed for clarity)
{
  "services": {
    "web":   { "namespace": "shop", "meshed": true },
    "cart":  { "namespace": "shop", "meshed": true }
  },
  "edges": [
    { "src": "web",  "dst": "cart", "rps": 42.7, "tls": true },
    { "src": "web",  "dst": "auth", "rps": 0.3,  "tls": false }
  ],
  "authPolicies": {
    "allow-web-to-cart": { ...CRDÂ specâ€¦ }
  }
}
```

A **delta** is a JSONâ€‘Patch array (`[{op:"add", path:"/edges/1", value:{â€¦}}]`).

---

## 5. Failure & Recovery Matrix

| Failure | Impact | Recovery path |
|---------|--------|---------------|
| **Collector pod OOM** | No new deltas; graph becomes stale after ~30â€¯s | Leader key expires â†’ standby wins within one expiry; continues publishing. |
| **Redis restart** | Snapshot + deltas lost | MCP servers fall back to local graph; first postâ€‘restart snapshot repopulates Redis. |
| **Prometheus down** | `rps/latency` fields freeze | Collector keeps topology-only updates; metrics gaps reflected as `stale=true` in API. |
| **K8s API throttles** | Informers behind | MCP reports `synced=false`; retries with exponential backâ€‘off. |

---

## 6. Deployment Footprint (helm values excerpt)

```yaml
mcp:
  replicas: 2
  image: ghcr.io/eli-nomasec/mcp-server:v0.3.0
  resources:
    requests: { cpu: "100m", memory: "200Mi" }
    limits:   { cpu: "500m", memory: "400Mi" }

collector:
  replicas: 2          # one active, one standby
  image: ghcr.io/eli-nomasec/mcp-collector:v0.3.0
  resources:
    requests: { cpu: "150m", memory: "250Mi" }

redis:
  enabled: true
  image: valkey/valkey:7-alpine
  resources:
    requests: { cpu: "30m", memory: "64Mi" }
  persistence: false   # RAMâ€‘only
```

---

## 7. Sequence Diagram â€“Â Successful `ApplyAuthorizationPolicy`

```mermaid
sequenceDiagram
    autonumber
    Client->>MCP: ApplyAuthorizationPolicy(spec)
    MCP->>K8sAPI: server-dry-run (validate)
    K8sAPI-->>MCP: 200 OK
    MCP->>K8sAPI: patch live CRD
    K8sAPI-->>MCP: 201 Created
    MCP-->>Client: ApplyResponse{accepted:true}

    Note over Collector,Redis: informer event after ~1Â s
    K8sAPI-)Collector: ADD AuthorizationPolicy
    Collector->>Redis: PUBLISH mesh:delta [{op:"add", path:"/authPolicies/allowâ€¦"}]
    Redis-)MCP: delta message
    MCP: updates inâ€‘mem graph
```

---

## 8. Implementation Checklist

1. **Repo scaffolding** (`cmd/collector`, `cmd/mcp-server`, `internal/graph`).  
2. Wire **k8s informer set** (generatorâ€‘based).  
3. PromQL client with rateâ€‘limited HTTP.  
4. Redis util: snapshotÂ â†”Â delta APIs, leader election helper.  
5. Protobuf & Buf config.  
6. gRPC server with interceptors (authZ, metrics).  
7. Helm chart + CI e2e on KIND.

---

> **You are now ready to code.**  
> The collector gathers and publishes; the MCP server consumes and serves.  
> All persistent groundâ€‘truth is still Kubernetes + Prometheus, so this stays light, faultâ€‘tolerant and *meshâ€‘native*.